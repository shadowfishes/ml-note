{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation  \n",
    "Here I will show hot to use it by lightGBM model.  \n",
    "Dataset used by kaggle competition on https://www.kaggle.com/c/ieee-fraud-detection\n",
    "\n",
    "\n",
    "# CV concept\n",
    "refers from https://www.kaggle.com/kyakovlev/ieee-cv-options\n",
    "## Basics\n",
    "Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data.\n",
    "\n",
    "In k-fold cross-validation, you split the input data into k subsets of data (also known as folds).\n",
    "\n",
    "## Main strategy\n",
    "1. Divide Train set in subsets (Training set itself + Validation set)\n",
    "2. Define Validation Metric (in our case it is ROC-AUC)\n",
    "3. Stop training when Validation metric stops improving\n",
    "4. Make predictions for Test set\n",
    "5. Seems simple but he devil's always in the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "TARGET = 'isFraud'\n",
    "train_df = pd.read_csv('./train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypterparams for lgb\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':5000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape control: (417559, 395) (83655, 395)\n"
     ]
    }
   ],
   "source": [
    "# dataset is time-series, so preparing it for training and split into two sets\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds=x)))\n",
    "train_df['DT_M'] = (train_df['DT_M'].dt.year - 2017) * 12 + train_df['DT_M'].dt.month\n",
    "\n",
    "# we use last month as varify data\n",
    "test_df = train_df[train_df['DT_M'] == train_df['DT_M'].max()].reset_index(drop=True)\n",
    "train_df = train_df[train_df['DT_M'] < (train_df['DT_M'].max())].reset_index(drop=True)\n",
    "\n",
    "print('Shape control:', train_df.shape, test_df.shape)\n",
    "\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype == 'O':\n",
    "        # print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col] = test_df[col].fillna('unseen_before_label')\n",
    "\n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col]) + list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "rm_cols = [\n",
    "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
    "    TARGET,                          # Not target in features))\n",
    "    'DT_M'                           # Column that we used to simulate test set\n",
    "]\n",
    "\n",
    "# Remove V columns (for faster training)\n",
    "rm_cols += ['V'+str(i) for i in range(1,340)]\n",
    "\n",
    "# Final features\n",
    "features_columns = [col for col in list(train_df.columns) if col not in rm_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 3 as number_splits for faster training\n",
    "N_SPLITS = 3\n",
    "RESULTS = test_df[['TransactionID',TARGET]]\n",
    "RESULTS[\"kfold\"] = 0\n",
    "train_x, train_y = train_df[features_columns], train_df[TARGET]\n",
    "test_x, test_y = test_df[features_columns], test_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounds 500  auc:  0.9288169585447247\n",
      "rounds 1000  auc:  0.9333970353168322\n",
      "rounds 1500  auc:  0.9334095099797453\n",
      "rounds 2000  auc:  0.9327899934117092\n"
     ]
    }
   ],
   "source": [
    "# no cv\n",
    "# We don't know when the model is best, so need grid-search for rounds\n",
    "\n",
    "for round in [500,1000,1500,2000]:\n",
    "    params = lgb_params.copy()\n",
    "    params[\"n_estimators\"] = round\n",
    "    params[\"early_stopping_rounds\"] = None\n",
    "    \n",
    "    train = lgb.Dataset(train_x, train_y)\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        params,\n",
    "        train,\n",
    "    )\n",
    "    pred = clf.predict(test_x)\n",
    "    print(\"rounds \"+str(round)+\"  auc: \", roc_auc_score(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.997964\tvalid_1's auc: 0.962431\n",
      "[2000]\ttraining's auc: 0.999892\tvalid_1's auc: 0.966918\n",
      "[3000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.967824\n",
      "Early stopping, best iteration is:\n",
      "[2983]\ttraining's auc: 0.999996\tvalid_1's auc: 0.967841\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.997748\tvalid_1's auc: 0.964897\n",
      "[2000]\ttraining's auc: 0.999889\tvalid_1's auc: 0.968642\n",
      "Early stopping, best iteration is:\n",
      "[2817]\ttraining's auc: 0.99999\tvalid_1's auc: 0.969374\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.997801\tvalid_1's auc: 0.960305\n",
      "[2000]\ttraining's auc: 0.999887\tvalid_1's auc: 0.964193\n",
      "[3000]\ttraining's auc: 0.999995\tvalid_1's auc: 0.96514\n",
      "Early stopping, best iteration is:\n",
      "[2995]\ttraining's auc: 0.999995\tvalid_1's auc: 0.965149\n",
      "kfold avg auc: 0.9310010973233493\n"
     ]
    }
   ],
   "source": [
    "# KFold/StratifiedKFold \n",
    "# The difference between these two is StratifiedKFold keep the ratio of target in each split.\n",
    "# Shuffle=True means shuffle the data before each epoch\n",
    "# I tried shuffle=True or not, besides it's different at valid_socre, it's simliar to each test_auc_score.\n",
    "# When set shuffle=True, we have a high valid_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "folds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "# folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(folds.split(train_x, train_y)):\n",
    "    tr_x, tr_y = train_x.loc[train_idx,:], train_y[train_idx]\n",
    "    tt_x, tt_y = train_x.loc[test_idx,:], train_y[test_idx]\n",
    "    \n",
    "    train = lgb.Dataset(tr_x, tr_y)\n",
    "    test = lgb.Dataset(tt_x, tt_y)\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        lgb_params,\n",
    "        train,\n",
    "        valid_sets = [train, test],\n",
    "        verbose_eval = 1000,\n",
    "    )\n",
    "    RESULTS[\"kfold\"] += clf.predict(test_x)/N_SPLITS\n",
    "\n",
    "print(\"kfold avg auc:\", roc_auc_score(test_y, RESULTS[\"kfold\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.997964\tvalid_1's auc: 0.92682\n",
      "Early stopping, best iteration is:\n",
      "[1277]\ttraining's auc: 0.999116\tvalid_1's auc: 0.927278\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.997748\tvalid_1's auc: 0.930164\n",
      "Early stopping, best iteration is:\n",
      "[1145]\ttraining's auc: 0.998546\tvalid_1's auc: 0.930523\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[751]\ttraining's auc: 0.995247\tvalid_1's auc: 0.927125\n"
     ]
    }
   ],
   "source": [
    "# LBO leave one block out \n",
    "# We use the last month of data as valid_data\n",
    "# We know the best round for training\n",
    "estimators_bestround = []\n",
    "\n",
    "for i , (tr_i, tt_i) in enumerate(folds.split(train_x,train_y)):\n",
    "    # print(tr_i[0:10])\n",
    "    tr_x, tr_y = train_x.loc[tr_i,:], train_y.loc[tr_i]\n",
    "    tt_x, tt_y = train_x.loc[tt_i,:], train_y.loc[tt_i]\n",
    "    train = lgb.Dataset(tr_x,tr_y)\n",
    "    #Here is different from above\n",
    "    test = lgb.Dataset(test_x,test_y)\n",
    "    clf = lgb.train(\n",
    "        lgb_params,\n",
    "        train,\n",
    "        valid_sets=[train,test],\n",
    "        verbose_eval=1000,\n",
    "    )\n",
    "    estimators_bestround.append(clf.current_iteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbo auc score:  0.9319127497119095\n"
     ]
    }
   ],
   "source": [
    "# Choose approximate rounds for reducing overfitting\n",
    "corrected_lgb_params = lgb_params.copy()\n",
    "corrected_lgb_params['n_estimators'] = int(np.mean(estimators_bestround))\n",
    "corrected_lgb_params['early_stopping_rounds'] = None\n",
    "\n",
    "RESULTS['lbo'] = 0\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x,train_y)):    \n",
    "    tr_x, tr_y = train_x.iloc[trn_idx, :], train_y[trn_idx]    \n",
    "    train = lgb.Dataset(tr_x, label=tr_y)\n",
    "    clf = lgb.train(\n",
    "        corrected_lgb_params,\n",
    "        train\n",
    "    )\n",
    "    RESULTS['lbo'] += clf.predict(test_x) / N_SPLITS\n",
    "\n",
    "print('lbo auc score: ', metrics.roc_auc_score(test_y, RESULTS['lbo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupKFold(n_splits=2)\n",
      "TRAIN: [2 3] TEST: [0 1 4 5]\n",
      "[[5 6]\n",
      " [7 8]] [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 9 10]\n",
      " [11 12]] [3 4] [1 2 5 6]\n",
      "TRAIN: [0 1 4 5] TEST: [2 3]\n",
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 9 10]\n",
      " [11 12]] [[5 6]\n",
      " [7 8]] [1 2 5 6] [3 4]\n"
     ]
    }
   ],
   "source": [
    "# GroupKFold refers from https://www.kaggle.com/kyakovlev/ieee-cv-options#GroupKFold\n",
    "# The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.\n",
    "\n",
    "# Why we may use it? Let's imagine that we want to separate train data by time blocks groups or client IDs or something else. \n",
    "# With GroupKFold we can be sure that our validation fold will contain groupIDs that are not in main train set. \n",
    "# Sometimes it helps to deal with \"dataleakage\" and overfit.\n",
    "\n",
    "# just a simulate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8],[9,10],[11,12]])\n",
    "y = np.array([1, 2, 3, 4,5,6])\n",
    "groups = np.array([0, 0, 1, 1, 2, 2])\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "group_kfold.get_n_splits(X, y, groups)\n",
    "print(group_kfold)\n",
    "GroupKFold(n_splits=2)\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
